---
output: 
  pdf_document: 
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
title: <h2> __Logistische Regression__ </h2>
author: "Xuan Son Le (4669361), Freie Universität Berlin"
date: "02/04/2018"
fontsize: 12pt
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
***  
__Abstract__: Im Rahmen der Abschlussarbeit des Moduls Programmieren mit R im Wintersemester 2017/2018 an der Freie Universität Berlin wird für diese Arbeit die statistische Methode namens binäres Logit-Modell ausgewählt. Diese Arbeit besteht aus zwei großen Hauptteilen: der Theorieteil, wobei die ausgewählte Methode theoretisch vorgestellt wird und der Implementierungsteil, welcher die Erklärung der Funktionalität vom selbst entwickelten Paket beinhaltet. Im Theorieteil wird zunächst ein Überblick über die grundliegende Funktionsweise vom (binären) Logit-Modell widergegeben. Die Grundidee von Generalisierten linearen Modellen wird anschließend kurz eingeführt, bevor der Aufbau vom binären Logit-Modell durch das Maximum Likelihood Verfahren vorgenommen wird. Demzufolge folgt die Interpretation der Koeffizienten vom binären Logit-Modell. Schließlich werden im Implementierungsteil alle Funktionen vom R-Paket schritterweise vorgestellt.  
  
**Keywords:** *Logit-Modell, logistische Regression, Paket, R*
  
***
\newpage

# Motivation

Die Anwendung von der klassischen linearen Regression ist für binäre (binomiale oder dichotome) Zielvariable (Response- oder zu erklärende Variable), welche lediglich zwei Werte (ja/nein, mänlich/weiblich, erfolgreich/nicht erfolgreich, etc.) annehmen kann, nicht mehr geeignet, da die Zielvariable von der linearen Regression metrisch skaliert ist. Oft wird binäre Variable als 0/1-Variable kodiert, das heißt sie nimmt nur den Wert 0 oder 1 an. Die folgende Grafik stellt den Ansatz graphisch dar, binäre Variable durch lineare Regression zu modellieren:

```{r echo=FALSE, fig.height=3, fig.width=6}
library(ggplot2)
data(cats, package = "MASS")
cats$Sex <- as.numeric(factor(cats$Sex, labels = c(0,1))) - 1
ggplot(cats, aes(y = Sex, x = Bwt)) + 
    geom_point(size = 2.6) +
    geom_smooth(method = "lm", se = F, fullrange = T) +
    scale_x_continuous(limits = c(1,4)) +
    scale_y_continuous(limits = c(-0.25,1.5), breaks = c(0,1)) +
    labs(x = "x1", y = "y")
```

Graphisch lässt sich festlegen, dass die lineare Regression den Wertebereich [0,1] von binären Responsevariablen sehr schnell verlässt. Wenn die Annahmen von der linearen Regression noch in Betracht gezogen werden, ergeben sich darüber hinaus noch folgende Probleme (vgl. ...):
*
*
*

Aus diesen Gründen wird ein ganz anderer Ansatz benötigt, um binäre Zielvariable zu modellieren, nämlich das binäre Logit-Modell, welches ebenfalls als binäre logistische Regression oder binäres logistisches Regressionsmodell bezeichnet werden kann. In der Statistik lassen sich Logit-Modelle noch in multinomiale und kumulative Logit-Modelle aufteilen, je nachdem ob die abhängige Variable multinominal- oder ordinalskaliert sind <QUELLE>. Diese Arbeit beschäftigt sich mit dem binären Logit-Modell, welches den Zusammenhang zwischen einer binären abhängigen Variable und einer/mehreren unabhängigen Variablen untersucht. Bei allen Arten von Logit-Modellen können die unabhängigen Variablen (erklärende oder Kovariablen) beliebig skaliert sein.

Im Unterschied zu der klassischen linearen Regression, welche den wahren Wert einer Zielvariable vorhersagt, interessiert sich das binäre Logit-Modell eher für die Wahrscheinlichkeit, dass die Zielvariable den Wert 1 annimmt. Das Hauptziel vom binären Logit-Modell ist es, die Wahrscheinlichkeit für den Eintritt der Zielvariable vorherzusagen. Dadurch soll die folgende theoretische Fragestellung beantwortet werden: *Wie stark ist der Einfluss von den unabhängigen (erklärenden) Variablen auf die Wahrscheinlichkeit, dass die abhängige (zu erklärende / Response) Variable eintritt beziehungsweise den Wert 1 annimmt?* In der Praxis kann diese Fragestellung beispielsweise so formuliert werden: "Haben Alter, Geschlecht, Berufe oder andere Merkmale der Kunden Einfluss auf die Wahrscheinlichkeit, dass sie ein Kredit rechtzeitig zurückzahlen?" oder "Lässt sich die Wahrscheinlichkeit, dass es regnet, durch die Temparatur, die Windstärke oder Sonnenstrahlungsintensität vorhersagen?". 

# Das binäre Logit-Modell

## Modellspezifikation

Das Logit-Modell ist eine Methode aus der Algorithmenklasse namens *Generalisierte Lineare Modelle* (engl. generalized linear model, kurz GLM), welche eine Verallgemeinerung des klassischen linearen Regressionsmodells anstrebt. Dazu gehören noch die klassische lineare Regression, Probitmodell und Poisson-Regression. Die Grundidee von GLM ist die Transformation der linearen Regressionsgleichung, so dass der Wertebereich der vorhergesagten Zielvariable dem gewünschten entspricht. Die Theorie von GLMs wurde von Nelder und Wedderburn entwickelt.

Gegeben seien n unabhängige Beobachtungen $y_1, y_2, ...,y_n$ der binären Zielvariable $\mathbf{Y}$. Ein Verteilungsmodell für $\mathbf{Y}$ ist die Binomialverteilung: $\mathbf{Y}_i \sim B(1, \pi_i)$ mit $\pi_i = P(Y_i = 1)$. Für diese Arbeit wird $\pi_i = (\pi_1, \pi_2, ..., \pi_n)$ als die Eintrittwahrscheinlichkeit von der einzelnen $\mathbf{Y}_i$ benannt. Weiterhin seien p erklärende Variablen $\mathbf{X}_0,\mathbf{X}_1,..,\mathbf{X}_p$ gegeben mit jeweils n unabhängigen Beobachtungen $\mathbf{X}_j = (x_{1j}, x_{2j},..., x_{nj})$ - j $\in$ {0,1,2,..,p} - gegeben. Daraus ergeben sich p Koeffizienten $\beta = (\beta_0, \beta_1, \beta_2,..., \beta_p)$, welche die Stärke den Zusammenhang zwischen die einzelne erklärende Variable mit der Zielvariable widerspiegeln. Dabei ist es sinnvoll, diese in einer Designmatrix $\mathbf{X}$ zu speichern. Da der Interzept ($\beta_0$) ebenfalls geschätzt werden soll, sind alle Werte der ersten Spalte von X gleich Eins, also $x_{10} = x_{20} = ... = x_{n0} = 1$. Zusammengefasst lässt sich die Designmatrix wie folgt darstellen: 
\[
\mathbf{X} =
 \begin{pmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p} \\
    1 & x_{21} & x_{22} & \cdots & x_{2p} \\
    1 & x_{31} & x_{32} & \cdots & x_{3p} \\
    \vdots  & \vdots  & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}
 \end{pmatrix}
\]
Die dazugehörige lineare Regressionsgleichung lautet: $\mathbf{Y} = \mathbf{X}.\beta + \epsilon$ mit $\epsilon = (\epsilon_1, \epsilon_2, \epsilon_3, ..., \epsilon_n)$ als die Abweichung der einzelnen Schätzungen gegenüber dem wahren Wert, wobei $\mathbf{Y}$ ein (nx1)-Vektor, $\mathbf{X}$ ein (nxp) und $\beta_i$ sowie $\epsilon_i$ ein (px1)-Vektor ist.

Die einzelne Beobachtung lässt sich wie folgt darstellen: 
$$
y_i = \beta_0 + \beta_1.x_{i1} + \beta_2.x_{i2} + ... + \beta_p.x_{ip} + \epsilon_i = \mathbf{x'}_i.\beta + \epsilon_i \qquad \forall_i = 1,2,3,...,n
$$ 

Wobei $\mathbf{x}_i$ der i-ten Zeile der Designmatrix $\mathbf{X}$ entspricht. Da bei der Multiplikation A.B die Regel gilt, dass die Spaltenanzahl von A der Zeilenanzahl von B entsprechen muss. Da $\beta$ ein (px1)-Vektor ist, muss $\mathbf{x}_i$ (px1-Vektor) in $\mathbf{x'}_i$ (1xp-Vektor) transponiert werden, damit die Multiplikation durchführbar ist.

Um die Werte im Bereich der reellen Zahlen von der linearen Regression auf dem Wertebereich von Wahrscheinlichkeiten zwischen 0 und 1 zu beschränken, sollte die rechte Seite der Gleichung transformiert werden. Das Ziel ist es, eine sinnvolle Verteilungsfunktion (Responsefunktion) zu finden, deren Wertebereich in [0,1] liegt: $\pi_i = P(\mathbf{Y_i} = 1) = F(\beta_0 + \beta_1.x_{i2} + \beta_2.x_{i3} + ... + \beta_p.x_{ip}) = F(\eta_i)$. Der lineare Prädikator $\eta_i = \beta_0 + \beta_1.x_{i2} + \beta_2.x_{i3} + ... + \beta_p.x_{ip} = \mathbf{x'}_i.\beta \ \ (1)$ wird ebenfalls als Linkfunktion genannt, weil dadurch eine Verbindung (Link) zwischen der Eintrittwahrscheinlichkeit und den unabhängigen Variablen erfolgt wird. Für das binäre Logit-Modell wird anstelle der Responsefunktion die standardisierte logistische Verteilung verwendet:
$$
F(\eta_i) = Logist(\eta_i) = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} \quad (2)
$$
Da durch die Responsefunktion die Eintrittwahrscheinlichkeit $\pi_i$ modelliert werden soll, ergibt sich die Gleichung für das binäre Logit-Modell wie folgt:
$$
\pi_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} = \frac{\exp(\beta_0 + \beta_1.x_{i2} + \beta_2.x_{i3} + ... + \beta_p.x_{ip})}{1 + \exp(\beta_0 + \beta_1.x_{i2} + \beta_2.x_{i3} + ... + \beta_p.x_{ip})} = \frac{\exp(\mathbf{x'}_i.\beta)}{1+\exp(\mathbf{x'}_i.\beta)} \quad (3)
$$
Dabei kann $\pi_i$ maximal den Wert 1 nehmen, wenn $\exp(\eta_i)$ sehr groß ist und minimal den Wert 0, wenn $\exp(\eta_i)$ sehr nah rechts von 0 liegt. $\exp(\eta_i)$ kann nicht negativ sein. Diese Gleichung erfüllt somit die Anforderung bezüglich dem Wertebereich von Wahrscheinlichkeiten.

Soll die Gleichung nach dem linearen Prädikator $\eta_i$ gelöst werden, ergibt sich schließlich die Logit-Linkfunktion:
$$
\begin{aligned}
\pi_i.(1 + \exp(\eta_i)) &= \exp(\eta_i) \\
\Leftrightarrow \pi_i + \pi_i.\exp(\eta_i) &= \exp(\eta_i) \\
\Leftrightarrow \pi_i &= \exp(\eta_i) - \pi_i.\exp(\eta_i)  \\
\Leftrightarrow \pi_i &= \exp(\eta_i).(1-\pi_i) \\
\Leftrightarrow \exp(\eta_i) &= \frac{\pi_i}{1-\pi_i} \\
\Leftrightarrow \eta_i &= \ln(\frac{\pi_i}{1-\pi_i}) \quad (4)\\
\end{aligned} 
$$

Es gilt nämlich:
$$
\beta_0 + \beta_1.x_{i2} + \beta_2.x_{i3} + ... + \beta_p.x_{ip} = \ln(\frac{\pi_i}{1-\pi_i}) \quad (5)
$$

## Maximum Likelihood Schätzung

Ähnlich wie bei der linearen Regression muss bei dem binären Logit-Modell die unbekannten Parameter $\beta_i$ (i = 0,1,2,...,k) ebenfalls geschätzt werden. Bei der klassischen linearen Regression wird die Methode der Kleinsten Quadrate (engl. *method of least squares*, kurz KQ-Methode) genutzt, um eine Regressionslinie zu bestimmen, welche die Summe der quadratischen Abweichungen von den beobachteten Punkten minimiert. Da bei dem binären Logit-Modell nicht der wahre Wert der Zielvariable sondern die Eintrittswahrscheinlichkeit geschätzt wird, ist die Abweichung zwischen dem wahren Wert und dem geschätzten Wert nicht mehr aussagekräftig wie bei der linearen Regression. Die Koeffizienten müssen anders geschätzt werden. Dementsprechend wird bei dem binären Logit-Modell die sogenannte Maximum Likelihood Schätzung (kurz ML-Schätzung) eingesetzt. Abbildung .. zeigt ein Beispiel mit zwei mögliche binäre logistiche Regressionskurven, die durch das Maximum Likelihood optimiert werden sollen.

```{r echo=FALSE, fig.height=3, fig.width=6}
library(ggplot2)
data(cats, package = "MASS")
cats$Sex <- as.numeric(as.character(factor(cats$Sex, labels = c(0,1))))
ggplot(cats, aes(y = Sex, x = Bwt)) + 
    geom_point(size = 2.6) +
    geom_smooth(method = "glm", se = F, fullrange = T,
                method.args = list(family = "binomial")) +
    geom_smooth(data = cats[1:95,], method = "glm", se = F, fullrange = T,
                method.args = list(family = "binomial"), color = "red") +
    scale_x_continuous(limits = c(1,4)) +
    scale_y_continuous(limits = c(-0.25,1.5), breaks = c(0,1)) +
    labs(x = "x1", y = "y")
```

Das Ziel der ML-Schätzung besteht darin, die Eintrittswahrscheinlichkeit für die empirischen Beobachtungswerte zu maximieren.Dafür kommt die (Log-)Likelihood-Funktion zum Einsatz. Im Folgenden wird die Vorgehensweise zum Aufbau der Log-Likelihood-Funktion wiedergegeben.

Gegeben sei $y_i = 1$ mit der Eintrittswahrscheinlichkeit $\pi_i$, und $y_i = 0$ mit der Gegenwahrscheinlichkeit $(1-\pi_i)$. Die Likelihood-Funktion lässt sich wie folgt definieren:
$$
\mathcal{L}(\beta) = {\prod_{i=1}^{n} \pi_i^{y_i}.(1-\pi_i)^{1-y_i}} \quad (6)
$$
Wenn $y_i$ gleich 1 ist, ergibt sich für die betroffene Beobachtung die Eintrittswahrscheinlichkeit $\pi_i$ und umgekehrt. Das Likelihood ist gleich die Multiplikation der Wahrscheinlichkeiten von allen Beobachtungen. Dieses soll maximiert werden.

Da $\pi_i$ von dem linearen Prädikator $\eta_i$ anhängt, ist die Likelihood-Funktion von $\beta$ abhängig. Wird $\pi_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}$ in die Likelihood-Funktion eingesetzt, ergibt sich:

$$
\mathcal{L}(\beta) = {\prod_{i=1}^{n} \Bigg[ \Big( \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} \Big)^{y_i}.\Big(1-\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\Big)^{1-y_i}}\Bigg] \quad (7)
$$

Der Versuch, Gleichung (6) zu differenzieren und nach $\beta$ zu lösen, um die Extremwerte zu finden, ist extrem aufwendig, weil sie eine Serie von Multiplikationen enthält. Wegen den exponentialen Komponenten kann die logistische Funktion aus der Mathematik zur Vereinfachung der Likelihood-Funktion Einsatz finden. Da die logistische Funktion eine monotone Funktion ist, entspricht jedes Maximum von der Likelihood-Funktion dem Maximum von der Log-Likelihood-Funktion und umgekehrt. Es gelgen für den Logarithmus folgende Regelungen (seien alle Vorzeichenvoraussetzungen für den Logarithmus erfüllt):
$$
\begin{aligned}
(8) \quad &\ln(\prod_{i=1}^{n}x_i) = \ln(x_1.x_2...x_n) = \ln(x_1) + \ln(x_2) + \ ... \ + \ln(x_n) = \sum_{i=1}^{n} \ln(x_i) \\
(9) \quad &\ln(x^\alpha) = \alpha.\ln(x) \\
(10) \quad &\ln(\frac{x}{y}) = \ln(x) - \ln(y)
\end{aligned} 
$$

Dementsprechend lässt die Likelihood-Funktion wie folgt logarithmisieren:
$$
\begin{aligned}
\ell(\beta) = \ln(\mathcal{L}(\beta)) = \quad &\ln \Bigg( \prod_{i=1}^{n} \pi_i^{y_i}.(1-\pi_i)^{1-y_i} \Bigg) \\
\mathrel{\overset{(8)}{=}} \quad &\sum_{i = 1}^{n} \ln \Big(\pi_i^{y_i}.(1-\pi_i)^{1-y_i}\Big) \\
\mathrel{\overset{(9)}{=}} \quad &\sum_{i = 1}^{n} \Big( y_i.\ln(\pi_i) + (1-y_i).\ln(1-\pi_i) \Big) \\
= \quad &\sum_{i = 1}^{n} \Big( y_i.\ln(\pi_i) - y_i.\ln(1-\pi_i) + \ln(1-\pi_i) \Big) \\
\mathrel{\overset{(10)}{=}} \quad &\sum_{i = 1}^{n} \Bigg( y_i.\ln \Big(\frac{\pi_i}{1-\pi_i}\Big) + \ln(1-\pi_i) \Bigg) \\
\mathrel{\overset{(4),(3)}{=}} \ &\sum_{i = 1}^{n} \Bigg( y_i.\eta_i + \ln \Big( 1- \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} \Big) \Bigg) \\
= \quad &\sum_{i = 1}^{n} \Bigg( y_i.\eta_i + \ln \Big( \frac{1}{1+\exp(\eta_i)}\Big) \Bigg) \\
= \quad &\sum_{i = 1}^{n} \Big( y_i.\eta_i - \ln (1 + exp(\eta_i)) \Big) \quad (11)
\end{aligned}
$$

Zum Herausfinden des ML-Schätzers, welcher die log-Likelihood-Funktion optimiert, wird Gleichung (10) nach $\beta$ differenziert. Die erste Ableitung von der log-Likelihood-Funktion wird als Score-Funktion benannt:

$$
\begin{aligned}
s(\beta) = \frac{\partial}{\partial \beta}  \ell(\beta) &= \frac{\partial}{\partial \beta} \sum_{i = 1}^{n} \Big( y_i.\eta_i - \ln (1 + \exp(\eta_i)) \Big) \quad \\
&\mathrel{\overset{(1)}{=}} \frac{\partial}{\partial \beta} \sum_{i = 1}^{n} \Big( y_i.\mathbf{x'}_i.\beta  - \ln (1 + \exp(\mathbf{x'}_i.\beta )) \Big) \quad \\ 
\end{aligned}
$$

Seien alle Vorzeichnenanforderungen erfüllt, gelten folgende Regelungen bezüglich der Differenzierungsrechnung:
$$
\begin{aligned}
&(12) \frac{\partial}{\partial t} a.f(t) = (a.f(t))' = a.f'(t) \\
&(13) \frac{\partial}{\partial t} \ln(f(t)) = [\ln(f(t))]' = \frac{f'(t)}{f(t)} \\ 
&(14) \frac{\partial}{\partial t} \exp(f(t)) = [\exp(f(t))]' = f'(t).\exp(f(t)) \\
\end{aligned}
$$

Eingesetzt in die Score-Funktion:
$$
\begin{aligned}
s(\beta) &= \sum_{i = 1}^{n} \Bigg( y_i.\mathbf{x'}_i - \frac{\mathbf{x'}_i.\exp(\mathbf{x'}_i.\beta)}{1 + \exp(\mathbf{x'}_i.\beta)} \Bigg) \\
&\mathrel{\overset{(1)}{=}}\sum_{i = 1}^{n} \Bigg[ \mathbf{x'}_i \Bigg( y_i - \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} \Bigg) \Bigg] \\
&\mathrel{\overset{(2)}{=}}  \sum_{i = 1}^{n} \ \mathbf{x'}_i ( y_i - \pi_i) \quad (15)
\end{aligned}
$$

Da $s(\beta)$ wegen exponentiellen Komponenten nicht linear von $\beta$ abhängt, wird zur Maximierung der Funktion ein iteratives Verfahren verwendet. Für diese Arbeit wird die Newton-Raphson-Methode ausgewählt. Dafür muss die zweite Ableitung der Funktion noch gebildet werden:

$$
\begin{aligned}
\frac{\partial^2}{\partial \beta^2} \ell(\beta) &= \frac{\partial}{\partial \beta} s(\beta) \\ 
&\mathrel{\overset{(15)}{=}} \frac{\partial}{\partial \beta} \sum_{i = 1}^{n} \ (\mathbf{x'}_i.y_i - \mathbf{x'}_i.\pi_i) \\
&= - \sum_{i = 1}^{n} \Bigg( \mathbf{x'}_i \ .  \Big(\frac{\partial}{\partial \beta} \pi_i \Big) \Bigg) \\
&\mathrel{\overset{(3)}{=}} - \sum_{i = 1}^{n} \Bigg( \mathbf{x'}_i \ .  \Big(\frac{\partial}{\partial \beta} \frac{\exp(\mathbf{x'}_i.\beta)}{1+\exp(\mathbf{x'}_i.\beta)} \Big) \Bigg) \\
\end{aligned}
$$

Wegen
$$
\frac{\partial}{\partial t} \frac{f(t)}{g(t)} = \frac{f'(t).g(t) - g'(t).f(t)}{(g(t))^2}
$$
gilt es:
$$
\begin{aligned}
\frac{\partial}{\partial \beta} s(\beta) &= - \sum_{i = 1}^{n} \Bigg[ \mathbf{x'}_i \ . \Bigg( \frac{\mathbf{x'}_i.\exp(\mathbf{x'}_i.\beta).(1+\exp(\mathbf{x'}_i.\beta))-\mathbf{x'}_i.\exp(\mathbf{x'}_i.\beta).\exp(\mathbf{x'}_i.\beta)}{(1+\exp(\mathbf{x'}_i.\beta))^2} \Bigg) \Bigg] \\
&= - \sum_{i = 1}^{n} \Bigg[ \mathbf{x'}_i \ . \frac{\mathbf{x'}_i.\exp(\mathbf{x'}_i.\beta)}{(1+\exp(\mathbf{x'}_i.\beta))^2}  \Bigg] \\
&= - \sum_{i = 1}^{n} \Bigg( \mathbf{x'}_i \ . \frac{\mathbf{x'}_i}{1+\exp(\mathbf{x'}_i.\beta)} . \frac{\exp(\mathbf{x'}_i.\beta)}{1+\exp(\mathbf{x'}_i.\beta)} \Bigg) \\
&\mathrel{\overset{(3)}{=}} - \sum_{i = 1}^{n} \Big( \mathbf{x'}_i . \mathbf{x'}_i . (1-\pi_i) . \pi_i \Big) \\
&= - \sum_{i = 1}^{n} \Big( (\mathbf{x'}_i)^2 . (1-\pi_i) . \pi_i \Big) \\
&= - \sum_{i = 1}^{n} \Big( \mathbf{x'}_i.\mathbf{x}_i . (1-\pi_i) . \pi_i \Big) \\
\end{aligned}
$$

## Intepretation der Koeffizienten
Die Bruchrechnung $\frac{\pi_i}{1-\pi_i}$ (siehe Gleichung 3) spielt bei dem binären Logit-Modell eine besondere Rolle, weil sie der Verbindung zwischen der Eintrittswahrscheinlichkeit und der Gegenwahrscheinlichkeit direkt widerspiegelt.

# Implementierung in R

Im Folgenden wird die Funktionalität von dem Paket **logitModell** erklärt, welches zum Ziel setzt, die Grundidee hinter dem binären Logit-Modell programmiert darzustellen. Das Paket besteht aus dem R-Code, welcher anhand dem manuell berechneten Maximum Likelihood ein Objekt von der Klasse *logitMod* erstellt und anschließend drei S3 Methoden für diese Klasse (print, summary und plot) definiert, und einer Vignette, welche den R-Code anhand einem konkreten Beispiel ausführt. Dieser Beispieldatensatz wird im Folgenden verwendet, um die Richtigkeit und Vollständigkeit der Ergebnisse der implementierten Methode im Vergleich zu der R-Standardmethode für Logit-Modell *glm(..., family = "binomial")* zu testen. Die binäre Responsevariable heißt *admit*, welche besagt ob ein Kandidat eine Zulassung bekommt. Zudem enthält der Datensatz drei unabhängige Variablen: *gre*, *gpa* (metrisch) und *rank* (kategorial). Der Datensatz soll ein Modell unterstützen, welche die Abhängigkeit von der Wahrscheinlichkeit einer Zulassung von der Abschlussnote, GRE-Note sowie dem Ruf von der angestrebten Institution. 

## Beispieldatensatz

Zunächst wird der Datensatz importiert. Dabei wird die Zielvariable aus dem Datensatz entnommen und in einem Vektor gespeichert. Da diese schon als 0/1-Variable vorgegeben wird, besteht es in diesem Fall keine Notwendigkeit, die Zielvariable zu faktorisieren. Der Code funktioniert allerdings ebenfalls mit Zielvariable, welche zum Beispiel als weiblich/männlich oder Erfolg/kein Erfolg kodiert wird und transformiert diese in eine 0/1-Variable. 

```{r eval = FALSE, results="hide"} 
# sei y die eingegebene Zielvariable
if (!(0 %in% y && 1 %in% y)) {
    y <- factor(y, labels = c(0,1))
}
y <- as.numeric(as.character(y))
```

Es muss immer vorab überprüft werden, in welcher Art die Zielvariable eingegeben wird, denn das Maximum Likelihood braucht als Input numerische Vektoren für weitere Berechnungen. Dieser Schritt wird extra gemacht, damit sich das manuelle Modell im Hinblick auf den Input gleich verhält wie das Standardmodell. 

## Implementierung der Maximum-Likelihood-Schätzung

Bevor das eigentliche Logit-Modell erstellt wird, wird in diesem Abschnitt die Implementierung der Maximum Likelihood Schätzung auseinandergesetzt. Der Code dazu ist auf Basis von dem betroffenen theoretischen Teil (siehe Abschnitt ...) aufgebaut. Die Funktion *maxLikeEst* dient dazu, anhand der Newton-Raphson-Methode das maximale Likelihood zu berechnen. Als Input werden ein Vektor (Zielvariable) und eine Matrix (Designmatrix) benötigt. Als Output wird ein Objekt erwartet, welches die Maximum-Likelihood-Schätzer enthält. Daneben werden ebenfalls die ersten notwendigen Kennwerte für den Aufbau von S3-Methoden in dem Objekt gespeichert.

Der Aufbau der Funktion maxLikeEst sieht so aus:
```{r}
maxLikeEst <- function(y, X) {
    
    #1. initialisiere Variablen 
    
    #2. berechne das Maximum Likelihood anhand Newton-Raphson-Schleife
    
    #3. berechne nötige Parameter und speichere diese in einem Objekt
    
}
```

Zunächst werden die Parameter und Variablen für die Newton-Raphson-Methode initialisiert:
```{r eval = FALSE, results="hide"}
# initialisiere beta
beta <- rep(0, times = ncol(X))

# initialisiere ...
M <- diag(nrow = nrow(X))

# stelle Abbruchskriterien fest
tolerance <- exp(-6)
diff <- 10 * abs(tolerance)
maxIteration <- 100
i <- 0
```

$\beta$ wird als ein Nullvektor mit der gleichen Länge wie die Anzahl der Spalten der Designmatrix initialisiert, um anschließend in der Newton-Raphson-Schleife nach jeder Iteration aktualisiert zu werden. Tolerance ist der Wert, bei dem die Schleife terminiert, wenn die absolute Veränderung der Lösungsgüte kleiner als dieser Wert ist. Das nächste Abbruchskriterium ist die maximale Anzahl der Iterationen. 

Solange die beiden Abbruchskriterien nicht erreicht sind, erfolgt in der Newton-Raphson-Schleife folgender iterativer Vorgang:

``` {r eval = FALSE, results="hide"}
while (diff > tolerance & i < maxIteration) {
        
    # berechne die Wahrscheinlichkeit
    eta <- X %*% beta
    exp_eta <- exp(eta)
    p <- as.vector(exp_eta / (1 + exp_eta))
    
    # aktualisiere ...
    M <- diag(p * (1 - p))
    
    # berechne die Änderung von Beta
    betaChange <- solve(t(X) %*% M %*% X) %*% t(X) %*% (y - p)
    
    # aktualisiere Beta
    beta <- beta + betaChange
    
    # berechne die totale Änderung von Beta
    diff <- sum(abs(betaChange))
    
    # nächste Iteration
    i <- i + 1
}
```

Die Änderung von $\beta$ führt dazu, dass nach jeder Iteration ebenfalls die Eintrittswahrscheinlichkeit sowie ... Matrix neu definiert werden müssen. 

Als Output der Funktion maxLikeEst werden folgende Parameter in ein Objekt gespeichert, um mit den Werten aus dem R-Standard-Modell zu vergleichen.

```{r}
# Freiheitsgrad = Anzahl an Beobachtungen - Anzahl an Parameter
dfRes <- nrow(X) - ncol(X)  # ResiduensFreiheitsgrad 
dfNull <- nrow(X) - 1       # Nullmodell Freiheitsgrad

# Devianz Residual
s <- y
s[s == 0] = -1
devianceResiduals = as.numeric(s * sqrt(-2*((y * eta) - (log(1 + exp(eta))))))
#devianceResiduals = -2 * as.numeric(crossprod(y,eta) - sum(log(1 + exp(eta))))    

# Kovarianzmatrix
vcov <- solve(t(X) %*% M %*% X) 

# Maximumwert der Log Likelihood Funktion
maxLogLikeValue <- (sum((y * X %*% beta) - (log(1 + exp(X %*% beta)))))
```

Für die Print-Methode werden die Freiheitsgrade, das Devianz-Residual und das Maximal-Likelihood bestimmt. Dieses wird demnächst in der Print-Methode näher erläutert. Zum Vergleich der Ergebnisse der manuell definierten Funktion mit dem Standardmodell werden die ML-Schätzer und die Kovarianzmatrix zurückgegeben. Dafür wird für das Paket eine Extra-Vignette in R aufgebaut, um die Ergebnisse anhand dem Beispieldatensatz zu verdeutlichen. Zum Aufruf der Extra-Vignette muss der folgende Code bei dem Errichten des Paketes ausgeführt werden:

```{r eval = FALSE, results = 'hide', warning = FALSE, message = FALSE}
setwd(".../logitModell") # der Pfad zu dem Paket
devtools::install(build_vignettes = TRUE)
vignette("logitModell")
```

## Definiere Klasse "logitMod"

Um anschließend mit S3-Methoden zu arbeiten, muss eine Klasse definiert werden, welche die Objekte aus der Funktion *maxLikeEst* umfasst. Diese Klasse wird als "logitMod" bezeichnet und wird wie folgt definiert:



\[
\begin{pmatrix}
    y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n 
 \end{pmatrix} = 
 \begin{pmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p} \\
    1 & x_{21} & x_{22} & \cdots & x_{2p} \\
    1 & x_{31} & x_{32} & \cdots & x_{3p} \\
    \vdots  & \vdots  & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}
 \end{pmatrix} \cdot
 \begin{pmatrix}
    \beta_1 \\ \beta_2 \\ \beta_3 \\ \vdots \\ \beta_p  
 \end{pmatrix} +
 \begin{pmatrix}
    \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n  
 \end{pmatrix}
\]